# ğŸ”§ ê°€ìƒí™˜ê²½ ì„¤ì • ë° ì‹¤í–‰ ë°©ë²•

## 1. ê°€ìƒí™˜ê²½ ìƒì„±

```bash
conda create -y -n rlenv python=3.10
conda activate rlenv

# í•„ìš” ì‹œ (ì¼ë¶€ í™˜ê²½ì—ì„œ site-packages ì¶©ëŒ ë°©ì§€)
export PYTHONNOUSERSITE=1

pip install -r requirements.txt


python run_policy.py

# Density-Matching GMM Policy Reinforcement Learning

ë³¸ í”„ë¡œì íŠ¸ëŠ” ê³ ì°¨ì› ì—°ì† ì œì–´ í™˜ê²½(Humanoid-v4)ì„ ëŒ€ìƒìœ¼ë¡œ,  
ì •ì±…ì„ **Gaussian Mixture Model(GMM)**ë¡œ í‘œí˜„í•˜ê³ ,  
Criticì´ ìœ ë„í•˜ëŠ” **Boltzmann í–‰ë™ ë°€ë„(Boltzmann Action Density)**ë¥¼  
ì •ì±…ì´ ì§ì ‘ ëª¨ë°©í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” **Density-Matching Reinforcement Learning ì•Œê³ ë¦¬ì¦˜**ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

ì´ë¥¼ í†µí•´ ê¸°ì¡´ Gaussian Policyê°€ ê°€ì§€ëŠ” í‘œí˜„ë ¥ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ,  
ë³´ë‹¤ ì•ˆì •ì ì´ê³  í’ë¶€í•œ í–‰ë™ ë¶„í¬ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

# ğŸ“˜ Reinforcement Learning Objective

ì•„ë˜ëŠ” ë³¸ í”„ë¡œì íŠ¸ì˜ ì „ì²´ Actorâ€“Critic í•™ìŠµ ìˆ˜ì‹ì„ ê°„ë‹¨í•˜ê²Œ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.

---

## ğŸ”¹ 1. Action Sampling

ê° ìƒíƒœ \( s \)ì—ì„œ Mê°œì˜ í–‰ë™ì„ standard Gaussianì—ì„œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤:

\[
a_i \sim \mathcal{N}(0, I)
\]

---

## ğŸ”¹ 2. Critic-Based Target Density

ìƒ˜í”Œ í–‰ë™ë“¤ì— ëŒ€í•´ Q-scoreë¥¼ ê³„ì‚°í•˜ê³ :

\[
Q_i = \min\big(Q_1(s,a_i), \; Q_2(s,a_i)\big)
\]

ì´ë¥¼ softmaxí•˜ì—¬ **critic-induced Boltzmann density**ë¥¼ ì–»ìŠµë‹ˆë‹¤:

\[
p_i = \frac{\exp(Q_i / T)}{\sum_j \exp(Q_j / T)}
\]

ì´ \( p_i \)ëŠ” ì •ì±…ì´ ë”°ë¼ê°€ì•¼ í•˜ëŠ” ì´ìƒì ì¸ í–‰ë™ ë¶„í¬(target density)ì…ë‹ˆë‹¤.

---

## ğŸ”¹ 3. GMM Policy Likelihood

ì •ì±…ì€ \( K \)ê°œì˜ Gaussian mixture componentë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

\[
\pi(a|s) = \sum_{k=1}^K w_k(s)
\,\mathcal{N}(a;\mu_k(s),\sigma_k^2(s))
\]

ê° í–‰ë™ì˜ log-likelihoodëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤:

\[
\log \pi(a_i|s)
\]

---

## ğŸ”¹ 4. Actor Loss (Density Matching)

ì •ì±…ì´ target density \( p_i \)ë¥¼ ëª¨ë°©í•˜ë„ë¡  
KL-divergence ê¸°ë°˜ ì†ì‹¤ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤:

\[
\mathcal{L}_{KL}
= - \sum_{i=1}^{M} p_i \log \pi(a_i|s)
\]

ì¶”ê°€ì ìœ¼ë¡œ entropy regularizationì„ ì ìš©í•˜ì—¬  
ì •ì±…ì´ ê³¼ë„í•˜ê²Œ ìˆ˜ì¶•í•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤:

\[
\mathcal{L}_{ent}
= -\lambda \sum_{k} w_k(s)\sum_{j} \log \sigma_{k,j}(s)
\]

ìµœì¢… Actor LossëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

\[
\mathcal{L}_{actor}
= \mathcal{L}_{KL} + \mathcal{L}_{ent}
\]

---

## ğŸ”¹ 5. Critic Loss (Soft Q-Learning)

Criticì€ Soft Q-learning ë°©ì‹ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.

Target Q-value:

\[
y = r + \gamma (1-d)\min\big(Q_1^{tgt}(s',a'), Q_2^{tgt}(s',a')\big)
\]

Critic Loss:

\[
\mathcal{L}_{critic}
= (Q_1(s,a)-y)^2 + (Q_2(s,a)-y)^2
\]

---

## ğŸ”¹ 6. Target Network Soft Update

í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ criticì˜ íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°±ì‹ í•©ë‹ˆë‹¤:

\[
\theta^{-} \leftarrow (1-\tau)\theta^{-} + \tau\theta
\]

ì´ soft updateëŠ” target criticì´ ëŠë¦¬ê²Œ ë³€í™”í•˜ë„ë¡ í•˜ì—¬  
overestimation ë° í•™ìŠµ ë¶ˆì•ˆì •ì„ ë°©ì§€í•©ë‹ˆë‹¤.

---

# âœ¨ Summary

ë³¸ ì•Œê³ ë¦¬ì¦˜ì€  
- **GMM ì •ì±…ì˜ í‘œí˜„ë ¥**,  
- **Density Matchingì˜ ì•ˆì •ì  í•™ìŠµ ì‹ í˜¸**,  
- **SAC ê¸°ë°˜ critic í•™ìŠµ**,  
- **Target Network EMA ì•ˆì •í™”**  

ë¥¼ ê²°í•©í•˜ì—¬, Humanoid-v4ì™€ ê°™ì€ ë³µì¡í•œ ì—°ì† ì œì–´ í™˜ê²½ì—ì„œ  
ê°•ë ¥í•˜ê³  ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

---

# ğŸ§© Repository Structure (ì˜ˆì‹œ)

