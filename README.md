# ğŸ”§ ê°€ìƒí™˜ê²½ ì„¤ì • ë° ì‹¤í–‰ ë°©ë²•

## 1. ê°€ìƒí™˜ê²½ ìƒì„±
```bash
conda create -y -n rlenv python=3.10
conda activate rlenv

# í•„ìš” ì‹œ (site-packages ì¶©ëŒ ë°©ì§€)
export PYTHONNOUSERSITE=1

pip install -r requirements.txt
```

## 2. ì‹¤í–‰
```bash
python run_policy.py
```

---

# ğŸ§  Density-Matching GMM Policy Reinforcement Learning

ë³¸ í”„ë¡œì íŠ¸ëŠ” ê³ ì°¨ì› ì—°ì† ì œì–´ í™˜ê²½(Humanoid-v4)ì„ ëŒ€ìƒìœ¼ë¡œ,  
ì •ì±…ì„ **Gaussian Mixture Model(GMM)**ë¡œ í‘œí˜„í•˜ê³ ,  
Criticì´ ìœ ë„í•˜ëŠ” **Boltzmann í–‰ë™ ë°€ë„**ë¥¼ ì •ì±…ì´ ì§ì ‘ ëª¨ë°©í•˜ë„ë¡ í•™ìŠµí•˜ëŠ”  
**Density-Matching Reinforcement Learning ì•Œê³ ë¦¬ì¦˜**ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

---

# ğŸ“˜ Reinforcement Learning Objective

## ğŸ”¹ 1. Action Sampling

ê° ìƒíƒœ $s$ì—ì„œ Mê°œì˜ í–‰ë™ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤:

$$a_i \sim \mathcal{N}(0, I)$$

## ğŸ”¹ 2. Critic-Based Target Density

$$Q_i = \min(Q_1(s,a_i), Q_2(s,a_i))$$

$$p_i = \frac{\exp(Q_i / T)}{\sum_j \exp(Q_j / T)}$$

## ğŸ”¹ 3. GMM Policy Likelihood

$$\pi(a|s) = \sum_{k=1}^{K} w_k(s) \, \mathcal{N}(a;\mu_k(s),\sigma_k^2(s))$$

## ğŸ”¹ 4. Actor Loss

$$\mathcal{L}_{\text{actor}} = - \sum_i p_i \log \pi(a_i|s) - \lambda \sum_k w_k(s) \sum_j \log \sigma_{k,j}(s)$$

## ğŸ”¹ 5. Critic Loss

$$y = r + \gamma(1-d)\min(Q_1^{\text{tgt}}(s',a'), Q_2^{\text{tgt}}(s',a'))$$

$$\mathcal{L}_{\text{critic}} = (Q_1 - y)^2 + (Q_2 - y)^2$$

## ğŸ”¹ 6. Target Network Soft Update

$$\theta^{-} \leftarrow (1-\tau)\theta^{-} + \tau\theta$$

---

## ğŸ“Š ì£¼ìš” íŠ¹ì§•

- **GMM ê¸°ë°˜ ì •ì±…**: ë‹¤ì¤‘ ëª¨ë“œ í–‰ë™ ë¶„í¬ í‘œí˜„
- **Density Matching**: Critic ê¸°ë°˜ Boltzmann ë¶„í¬ ì§ì ‘ ëª¨ë°©
- **Double Q-Learning**: ê³¼ëŒ€í‰ê°€ ë°©ì§€
- **Soft Target Update**: ì•ˆì •ì ì¸ í•™ìŠµ
